{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a07246ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.models import DenseNet121_Weights, ResNet101_Weights\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b7ede7",
   "metadata": {},
   "source": [
    "### Load and Evaluate Default Models\n",
    "\n",
    "Note that ResNet and DenseNet are both explored in previous works examining the structure of loss functions. Pretrained models are available for both through pytorch. However, these are trained in the ImageNet1K dataset which is extremely large (1.2M images). It is tbd whether I load a small portion of ImageNet1K or fine-tune both models on CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "332673dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR dataset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),  # needed for ImageNet-pretrained models\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=64, shuffle=False, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1302b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained models\n",
    "densenet = models.densenet121(weights=DenseNet121_Weights.IMAGENET1K_V1)\n",
    "resnet = models.resnet101(weights=ResNet101_Weights.IMAGENET1K_V2)\n",
    "\n",
    "# Edit the output size\n",
    "densenet.classifier = torch.nn.Linear(densenet.classifier.in_features, 10)\n",
    "resnet.fc = torch.nn.Linear(resnet.fc.in_features, 10)\n",
    "\n",
    "densenet = densenet.to(device)\n",
    "resnet = resnet.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a9f9080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model, loader, device, num_epochs=100, type=None):\n",
    "    \"\"\" Train the last classifier layer to align the models with the CIFAR10 dataset \"\"\"\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters())\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Set minority of weights to be trainable (linear maps)\n",
    "\n",
    "    model.eval()\n",
    "    if type == \"densenet\":\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = name.startswith(\"classifier\")\n",
    "        model.classifier.train()\n",
    "    elif type == \"resnet\":\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = name.startswith(\"fc\")\n",
    "        model.fc.train()\n",
    "    else:\n",
    "        raise Exception\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    # Fine-tune on the training set for a given number of epochs\n",
    "    for epoch, (image, labels) in tqdm(enumerate(loader)):\n",
    "        if epoch > num_epochs: break\n",
    "        images, labels = image.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() \n",
    "\n",
    "        if epoch % 10 == 0 and epoch != 0:\n",
    "            print(f\"Epoch {epoch}: Loss {total_loss}\")\n",
    "            total_loss = 0\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    \"\"\" Basic evaluation script \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc8d3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For both models, finetune, evaluate, and save\n",
    "finetune(densenet, trainloader, device, num_epochs=500, type=\"densenet\")\n",
    "densenet_acc = evaluate(densenet, testloader, device)\n",
    "print(densenet_acc)\n",
    "torch.save(densenet.state_dict(), 'models/densenet_cifar10.pt')\n",
    "\n",
    "finetune(resnet, trainloader, device, num_epochs=500, type=\"resnet\")\n",
    "resnet_acc = evaluate(resnet, testloader, device)\n",
    "print(resnet_acc)\n",
    "torch.save(resnet.state_dict(), 'models/resnet_cifar10.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46af6ea1",
   "metadata": {},
   "source": [
    "### Sample from the Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d84c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select random directions in weight space to select weights\n",
    "\n",
    "# Select new weights from a normal distribution centered at the actual weights\n",
    "\n",
    "# Evaluate the loss function at these points\n",
    "\n",
    "# Save this set of points (we can use this for both modeling approaches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9188b8db",
   "metadata": {},
   "source": [
    "### Train Geometric AutoEncoder to Approximate Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c2383a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e36c3b4",
   "metadata": {},
   "source": [
    "### Map Loss with Standard and Geometric AutoEncoder Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cd6647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
